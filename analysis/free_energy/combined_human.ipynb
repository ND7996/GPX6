{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0333e382",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base (Python 3.10.14)' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/home/hp/results/HUMAN/distance_analysis_data.csv\")\n",
    "print(\"Columns in the CSV:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb39404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check to count levels in your dataset\n",
    "import pandas as pd\n",
    "\n",
    "def check_levels(csv_path):\n",
    "    \"\"\"Quick function to check how many levels are in the dataset\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Clean Level column (same logic as in your main code)\n",
    "    if 'Level' in df.columns:\n",
    "        df['Level'] = df['Level'].astype(str)\n",
    "        \n",
    "        def extract_level_number(level_str):\n",
    "            if level_str.startswith('level'):\n",
    "                return int(level_str.replace('level', ''))\n",
    "            else:\n",
    "                try:\n",
    "                    return int(float(level_str))\n",
    "                except ValueError:\n",
    "                    return 0\n",
    "        \n",
    "        df['Level'] = df['Level'].apply(extract_level_number)\n",
    "        \n",
    "        unique_levels = sorted(df['Level'].unique())\n",
    "        print(f\"Number of unique levels: {len(unique_levels)}\")\n",
    "        print(f\"Level range: {min(unique_levels)} to {max(unique_levels)}\")\n",
    "        print(f\"All levels: {unique_levels}\")\n",
    "        \n",
    "        return len(unique_levels), unique_levels\n",
    "    else:\n",
    "        print(\"No 'Level' column found in the dataset\")\n",
    "        return 0, []\n",
    "\n",
    "# Usage:\n",
    "csv_path = \"/home/hp/results/HUMAN/distance_analysis_data.csv\"\n",
    "num_levels, all_levels = check_levels(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d03e735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import rcParams\n",
    "import seaborn as sns\n",
    "import matplotlib.font_manager as fm\n",
    "from scipy import stats\n",
    "import os\n",
    "\n",
    "def normalize_data(data):\n",
    "    \"\"\"\n",
    "    Normalize data to 0-1 range using min-max normalization\n",
    "    Replaces sklearn's MinMaxScaler\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : numpy.ndarray\n",
    "        Data to normalize\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Normalized data with values between 0 and 1\n",
    "    \"\"\"\n",
    "    data_min = np.min(data, axis=0)\n",
    "    data_max = np.max(data, axis=0)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    data_range = data_max - data_min\n",
    "    data_range[data_range == 0] = 1\n",
    "    \n",
    "    return (data - data_min) / data_range\n",
    "\n",
    "def check_arial_font():\n",
    "    \"\"\"Check if Arial font is available and provide installation guidance\"\"\"\n",
    "    available_fonts = [f.name for f in fm.fontManager.ttflist]\n",
    "    arial_variants = ['Arial', 'Arial Unicode MS', 'Arial Black']\n",
    "    \n",
    "    arial_available = any(font in available_fonts for font in arial_variants)\n",
    "    \n",
    "    if not arial_available:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def setup_plos_style():\n",
    "    \"\"\"Configure matplotlib for PLOS ONE journal standards with larger fonts\"\"\"\n",
    "    # Check Arial availability first\n",
    "    arial_available = check_arial_font()\n",
    "    \n",
    "    # Set font family with fallbacks\n",
    "    if arial_available:\n",
    "        rcParams['font.family'] = 'Arial'\n",
    "    else:\n",
    "        # Use best available alternative\n",
    "        available_fonts = [f.name for f in fm.fontManager.ttflist]\n",
    "        fallback_fonts = ['DejaVu Sans', 'Liberation Sans', 'Helvetica', 'sans-serif']\n",
    "        \n",
    "        for font in fallback_fonts:\n",
    "            if font in available_fonts:\n",
    "                rcParams['font.family'] = font\n",
    "                break\n",
    "        else:\n",
    "            rcParams['font.family'] = 'sans-serif'\n",
    "    \n",
    "    # PLOS ONE compliant font sizes\n",
    "    rcParams['font.size'] = 12\n",
    "    rcParams['axes.labelsize'] = 14\n",
    "    rcParams['axes.titlesize'] = 16\n",
    "    rcParams['xtick.labelsize'] = 12\n",
    "    rcParams['ytick.labelsize'] = 12\n",
    "    rcParams['legend.fontsize'] = 10\n",
    "    rcParams['figure.titlesize'] = 18\n",
    "    rcParams['axes.linewidth'] = 1.0\n",
    "    rcParams['xtick.major.width'] = 1.0\n",
    "    rcParams['ytick.major.width'] = 1.0\n",
    "    rcParams['xtick.minor.width'] = 0.5\n",
    "    rcParams['ytick.minor.width'] = 0.5\n",
    "    rcParams['savefig.dpi'] = 300\n",
    "    rcParams['savefig.bbox'] = 'tight'\n",
    "    rcParams['savefig.pad_inches'] = 0.1\n",
    "\n",
    "def get_human_msa_position_colors():\n",
    "    \"\"\"\n",
    "    Define colors for each position - HUMAN specific positions\n",
    "    These colors match the HUMAN MSA visualization\n",
    "    \"\"\"\n",
    "    # Enhanced color palette with better contrast - SAME AS MOUSE\n",
    "    color_palette = [\n",
    "        \"#E41A1C\", \"#377EB8\", \"#4DAF4A\", \"#984EA3\", \"#FF7F00\", \"#FFFF33\",\n",
    "        \"#A65628\", \"#F781BF\", \"#1B9E77\", \"#D95F02\", \"#7570B3\", \"#66A61E\",\n",
    "        \"#E31A1C\", \"#1F78B4\", \"#33A02C\", \"#FB9A99\", \"#CAB2D6\", \"#FDBF6F\",\n",
    "        \"#00CED1\", \"#FF6347\", \"#32CD32\", \"#FFB6C1\", \"#20B2AA\"  # Additional colors for extra positions\n",
    "    ]\n",
    "    \n",
    "    # HUMAN positions to mark - UPDATE THIS LIST BASED ON YOUR HUMAN DATA\n",
    "    # You'll need to replace this with the actual positions found in your HUMAN dataset\n",
    "    positions_to_mark = [3, 4, 48, 49, 52, 47, 99, 54, 177, 144, 178, 74, 143, 139, 87, 142, 102, 104, 107, 24, 60, 181, 173]\n",
    "    \n",
    "    # Create position-color mapping\n",
    "    position_colors = {pos: color_palette[i % len(color_palette)] for i, pos in enumerate(positions_to_mark)}\n",
    "    \n",
    "    return position_colors\n",
    "\n",
    "def extract_position_from_mutation(mutation):\n",
    "    \"\"\"Extract position number from mutation string - Updated for HUMAN format\"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Handle HUMAN mutation formats (similar to MOUSE: e.g., C49U, F48Y, T52A, etc.)\n",
    "    # Try to extract number from mutation string\n",
    "    numbers = re.findall(r'\\d+', str(mutation))\n",
    "    if numbers:\n",
    "        return int(numbers[0])\n",
    "    \n",
    "    # If no number found, try other patterns\n",
    "    if 'Position' in str(mutation):\n",
    "        parts = str(mutation).split('Position')\n",
    "        if len(parts) > 1:\n",
    "            nums = re.findall(r'\\d+', parts[1])\n",
    "            if nums:\n",
    "                return int(nums[0])\n",
    "    \n",
    "    return None\n",
    "\n",
    "def extract_level_number(level_str):\n",
    "    \"\"\"Extract numeric level from level string (e.g., 'level0' -> 0)\"\"\"\n",
    "    import re\n",
    "    \n",
    "    if pd.isna(level_str):\n",
    "        return None\n",
    "    \n",
    "    # Convert to string if not already\n",
    "    level_str = str(level_str).lower()\n",
    "    \n",
    "    # Extract number from level string\n",
    "    numbers = re.findall(r'\\d+', level_str)\n",
    "    if numbers:\n",
    "        return int(numbers[0])\n",
    "    \n",
    "    return None\n",
    "\n",
    "def find_pathway_mutations_dg_star_only(df_sorted):\n",
    "    \"\"\"\n",
    "    Find the pathway mutations for ΔG‡ only\n",
    "    For ΔG‡: mutation with LOWEST value (best kinetics)\n",
    "    Returns dataframe with pathway mutations sorted by level\n",
    "    \"\"\"\n",
    "    pathway_mutations_dg_star = []  # For ΔG‡ pathway (lowest values)\n",
    "    \n",
    "    # Group by level and find pathway mutations for ΔG‡ only\n",
    "    for level in sorted(df_sorted['Level_Numeric'].unique()):\n",
    "        level_data = df_sorted[df_sorted['Level_Numeric'] == level]\n",
    "        \n",
    "        # For ΔG‡ pathway: Find mutation with minimum ΔG‡ (lowest activation energy)\n",
    "        min_dg_star_idx = level_data['dg_star'].idxmin()\n",
    "        best_mutation_dg_star = level_data.loc[min_dg_star_idx]\n",
    "        pathway_mutations_dg_star.append(best_mutation_dg_star)\n",
    "        \n",
    "        print(f\"Level {level}:\")\n",
    "        print(f\"  ΔG‡ pathway: {best_mutation_dg_star['mutation']} at position {best_mutation_dg_star['Position']} (ΔG‡ = {best_mutation_dg_star['dg_star']:.2f})\")\n",
    "    \n",
    "    pathway_df_dg_star = pd.DataFrame(pathway_mutations_dg_star)\n",
    "    \n",
    "    return pathway_df_dg_star\n",
    "\n",
    "def create_plots_bc_with_pathway(csv_path):\n",
    "    \"\"\"\n",
    "    Create plots with pathway line ONLY on ΔG‡ plot\n",
    "    Plot for ΔG° will have no pathway line\n",
    "    Removes B and C labels from titles\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    csv_path : str\n",
    "        Path to CSV file containing mutation data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Setup PLOS ONE styling\n",
    "    setup_plos_style()\n",
    "    \n",
    "    # Load and validate data\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"Loaded {len(df)} rows from CSV\")\n",
    "        print(f\"Columns: {list(df.columns)}\")\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Could not find file: {csv_path}\")\n",
    "    \n",
    "    # Standardize column names\n",
    "    column_mapping = {\n",
    "        'Mean_dG_star': 'dg_star',\n",
    "        'Mean_dG0': 'dg0', \n",
    "        'Mutation': 'mutation',\n",
    "        'mean_dg_star': 'dg_star',\n",
    "        'mean_dg0': 'dg0'\n",
    "    }\n",
    "    \n",
    "    for old_name, new_name in column_mapping.items():\n",
    "        if old_name in df.columns:\n",
    "            df = df.rename(columns={old_name: new_name})\n",
    "    \n",
    "    # Validate required columns\n",
    "    required_cols = {'dg_star', 'dg0', 'mutation', 'Level'}\n",
    "    missing_cols = required_cols - set(df.columns)\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "    \n",
    "    # Clean and sort data - first extract numeric levels\n",
    "    print(f\"Original Level values: {df['Level'].unique()}\")\n",
    "    df['Level_Numeric'] = df['Level'].apply(extract_level_number)\n",
    "    print(f\"Extracted numeric levels: {sorted(df['Level_Numeric'].dropna().unique())}\")\n",
    "    \n",
    "    # Remove rows with missing critical data\n",
    "    df_clean = df.dropna(subset=['dg_star', 'dg0', 'mutation', 'Level_Numeric']).copy()\n",
    "    print(f\"After cleaning: {len(df_clean)} rows remaining\")\n",
    "    \n",
    "    df_sorted = df_clean.sort_values('Level_Numeric').reset_index(drop=True)\n",
    "    \n",
    "    # Extract positions from mutations\n",
    "    df_sorted['Position'] = df_sorted['mutation'].apply(extract_position_from_mutation)\n",
    "    print(f\"Positions found: {sorted(df_sorted['Position'].dropna().unique())}\")\n",
    "    \n",
    "    # Find pathway mutations (ONLY for ΔG‡)\n",
    "    pathway_df_dg_star = find_pathway_mutations_dg_star_only(df_sorted)\n",
    "    print(f\"\\nΔG‡ pathway mutations found: {len(pathway_df_dg_star)}\")\n",
    "    \n",
    "    # Get unique positions for legend\n",
    "    unique_positions = sorted([pos for pos in df_sorted['Position'].unique() if pos is not None])\n",
    "    \n",
    "    # Get HUMAN MSA position colors\n",
    "    human_msa_position_colors = get_human_msa_position_colors()\n",
    "    \n",
    "    # Assign colors based on position - using HUMAN MSA colors\n",
    "    df_sorted['Color'] = df_sorted['Position'].map(human_msa_position_colors)\n",
    "    # Use gray for positions not in MSA color scheme\n",
    "    df_sorted['Color'] = df_sorted['Color'].fillna('#808080')\n",
    "    \n",
    "    # Also assign colors to pathway mutations\n",
    "    pathway_df_dg_star['Color'] = pathway_df_dg_star['Position'].map(human_msa_position_colors)\n",
    "    pathway_df_dg_star['Color'] = pathway_df_dg_star['Color'].fillna('#808080')\n",
    "    \n",
    "    print(f\"HUMAN MSA Color mapping applied:\")\n",
    "    for pos in sorted(df_sorted['Position'].dropna().unique()):\n",
    "        if pos in human_msa_position_colors:\n",
    "            print(f\"  Position {pos}: {human_msa_position_colors[pos]} (MSA color)\")\n",
    "        else:\n",
    "            print(f\"  Position {pos}: #808080 (default gray)\")\n",
    "    \n",
    "    # Create figure with 1x2 layout\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8), facecolor='white')\n",
    "    fig.subplots_adjust(left=0.08, right=0.75, bottom=0.15, top=0.85, wspace=0.3)\n",
    "    \n",
    "    # Use numeric levels for plotting\n",
    "    all_levels = sorted(df_sorted['Level_Numeric'].unique())\n",
    "    print(f\"Plotting levels: {all_levels}\")\n",
    "    \n",
    "    # Plot 1: Level vs ΔG° with ALIGNED dots (NO JITTER) - NO PATHWAY LINE\n",
    "    for idx, row in df_sorted.iterrows():\n",
    "        # Use exact x-coordinate - NO JITTER for perfect vertical alignment\n",
    "        ax1.scatter(row['Level_Numeric'], row['dg0'], \n",
    "                   c=row['Color'], s=120, alpha=0.8,\n",
    "                   edgecolors='black', linewidth=1.0)\n",
    "    \n",
    "    # NO pathway line for ΔG° plot\n",
    "    \n",
    "    ax1.set_xticks(all_levels)\n",
    "    ax1.set_xlabel('Mutation Level', fontweight='bold', fontsize=14)\n",
    "    ax1.set_ylabel('ΔG° (kcal/mol)', fontweight='bold', fontsize=14)\n",
    "    ax1.set_title('Mutation Level vs ΔG°', fontweight='bold', fontsize=10, pad=20)\n",
    "    ax1.grid(True, alpha=0.3, linewidth=0.5)\n",
    "    \n",
    "    # Plot 2: Level vs ΔG‡ with ALIGNED dots (NO JITTER) - WITH PATHWAY LINE\n",
    "    for idx, row in df_sorted.iterrows():\n",
    "        # Use exact x-coordinate - NO JITTER for perfect vertical alignment\n",
    "        ax2.scatter(row['Level_Numeric'], row['dg_star'], \n",
    "                   c=row['Color'], s=120, alpha=0.8,\n",
    "                   edgecolors='black', linewidth=1.0)\n",
    "    \n",
    "    # Add pathway line for ΔG‡ plot - connects LOWEST ΔG‡ mutations\n",
    "    pathway_x_dg_star = pathway_df_dg_star['Level_Numeric'].values\n",
    "    pathway_y_dg_star = pathway_df_dg_star['dg_star'].values\n",
    "    ax2.plot(pathway_x_dg_star, pathway_y_dg_star, 'k-', linewidth=3, alpha=0.7, label='Evolutionary Pathway')\n",
    "    \n",
    "    # Highlight ΔG‡ pathway points (lowest ΔG‡ mutations)\n",
    "    for idx, row in pathway_df_dg_star.iterrows():\n",
    "        ax2.scatter(row['Level_Numeric'], row['dg_star'], \n",
    "                   c=row['Color'], s=200, alpha=1.0,\n",
    "                   edgecolors='black', linewidth=2.5, marker='s')  # Square markers for pathway\n",
    "    \n",
    "    ax2.set_xticks(all_levels)\n",
    "    ax2.set_xlabel('Mutation Level', fontweight='bold', fontsize=14)\n",
    "    ax2.set_ylabel('ΔG‡ (kcal/mol)', fontweight='bold', fontsize=14)\n",
    "    ax2.set_title('Mutation Level vs ΔG‡ (Lowest ΔG‡ Pathway)', fontweight='bold', fontsize=10, pad=20)\n",
    "    ax2.grid(True, alpha=0.3, linewidth=0.5)\n",
    "    \n",
    "    # Clean up axes - remove top and right spines\n",
    "    for ax in [ax1, ax2]:\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['left'].set_linewidth(1.5)\n",
    "        ax.spines['bottom'].set_linewidth(1.5)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=12, width=1.5, length=6)\n",
    "        ax.tick_params(axis='both', which='minor', labelsize=10, width=1, length=3)\n",
    "    \n",
    "    # Create legend with HUMAN MSA position colors\n",
    "    legend_elements = []\n",
    "    \n",
    "    # Only show positions that have MSA colors\n",
    "    msa_positions_in_data = [pos for pos in unique_positions if pos in human_msa_position_colors]\n",
    "    other_positions = [pos for pos in unique_positions if pos not in human_msa_position_colors]\n",
    "    \n",
    "    # Add MSA colored positions first\n",
    "    for pos in sorted(msa_positions_in_data):\n",
    "        legend_elements.append(plt.Line2D([0], [0], marker='o', color='w', \n",
    "                                        markerfacecolor=human_msa_position_colors[pos], \n",
    "                                        markersize=10, markeredgecolor='black',\n",
    "                                        markeredgewidth=1, label=f'Position {pos}'))\n",
    "    \n",
    "    # Add other positions in gray\n",
    "    for pos in sorted(other_positions):\n",
    "        legend_elements.append(plt.Line2D([0], [0], marker='o', color='w', \n",
    "                                        markerfacecolor='#808080', \n",
    "                                        markersize=10, markeredgecolor='black',\n",
    "                                        markeredgewidth=1, label=f'Position {pos}'))\n",
    "    \n",
    "    # Add ONLY the pathway line to legend\n",
    "    legend_elements.append(plt.Line2D([0], [0], color='black', linewidth=3, alpha=0.7, label='Evolutionary Pathway'))\n",
    "    \n",
    "    # Add legend outside the plot area\n",
    "    if legend_elements:\n",
    "        # Split legend into multiple columns if too many positions\n",
    "        ncol = 3 if len(legend_elements) > 10 else 2\n",
    "        legend = fig.legend(handles=legend_elements, \n",
    "                          bbox_to_anchor=(0.77, 0.5), \n",
    "                          loc='center left',\n",
    "                          ncol=ncol,\n",
    "                          fontsize=10,\n",
    "                          title='Position Legend',\n",
    "                          title_fontsize=12,\n",
    "                          frameon=True,\n",
    "                          fancybox=True,\n",
    "                          shadow=True)\n",
    "        legend.get_title().set_fontweight('bold')\n",
    "    \n",
    "    # Add main figure title\n",
    "    fig.suptitle('HUMAN Mutation Analysis', \n",
    "                 fontsize=12, fontweight='bold', y=0.92)\n",
    "    \n",
    "    # Save the figure to both specified paths\n",
    "    save_paths = [\n",
    "        '/home/hp/nayanika/github/GPX6/figures/human_mutation_analysis.png',\n",
    "        '/home/hp/nayanika/github/Article-GPX6-EVB/Figures/human_mutation_analysis.png'\n",
    "    ]\n",
    "    \n",
    "    for save_path in save_paths:\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        \n",
    "        # Save the figure\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "        print(f\"Figure saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"HUMAN MUTATION ANALYSIS - ΔG‡ EVOLUTIONARY PATHWAY ONLY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total mutations analyzed: {len(df_sorted)}\")\n",
    "    print(f\"Unique positions found: {len(unique_positions)}\")\n",
    "    print(f\"Levels analyzed: {all_levels}\")\n",
    "    print(f\"ΔG‡ pathway mutations: {len(pathway_df_dg_star)}\")\n",
    "    print(f\"✓ HUMAN-specific positions in color scheme!\")\n",
    "    print(f\"✓ Dots perfectly aligned in vertical lines!\")\n",
    "    print(f\"✓ ΔG‡ pathway connects LOWEST ΔG‡ mutations (best kinetics)!\")\n",
    "    print(f\"✓ ΔG° plot has NO pathway line!\")\n",
    "    print(f\"✓ Pathway only shown on ΔG‡ plot!\")\n",
    "    print(f\"✓ B and C labels removed from titles!\")\n",
    "    print(f\"✓ Figure saved to both specified paths!\")\n",
    "    \n",
    "    # Show pathway summary for ΔG‡ only\n",
    "    print(f\"\\nΔG‡ Evolutionary Pathway Summary (Lowest ΔG‡ per Level - Best Kinetics):\")\n",
    "    print(f\"{'Level':<8} {'Mutation':<12} {'Position':<10} {'ΔG°':<12} {'ΔG‡':<12}\")\n",
    "    print(\"-\" * 60)\n",
    "    for idx, row in pathway_df_dg_star.iterrows():\n",
    "        print(f\"{row['Level_Numeric']:<8} {row['mutation']:<12} {row['Position']:<10} {row['dg0']:<12.2f} {row['dg_star']:<12.2f}\")\n",
    "    \n",
    "    return fig, (ax1, ax2), df_sorted, pathway_df_dg_star\n",
    "\n",
    "# Main function for plotting with pathway on ΔG‡ only\n",
    "def main():\n",
    "    \"\"\"Main function for plotting with evolutionary pathway ONLY on ΔG‡\"\"\"\n",
    "    csv_path = \"/home/hp/results/HUMAN/distance_analysis_data.csv\"  # UPDATE PATH FOR HUMAN\n",
    "    \n",
    "    print(\"Creating HUMAN plots with ΔG‡ evolutionary pathway only...\")\n",
    "    print(f\"✓ Connecting mutations with LOWEST ΔG‡ in each level!\")\n",
    "    print(\"✓ Pathway follows most favorable energy barriers!\")\n",
    "    print(\"✓ Using HUMAN-specific color scheme!\")\n",
    "    print(\"✓ ΔG° plot has NO pathway line!\")\n",
    "    print(\"✓ B and C labels removed from titles!\")\n",
    "    print(\"✓ Figure will be saved to both specified paths!\")\n",
    "    \n",
    "    try:\n",
    "        fig, axes, df_analysis, pathway_df_dg_star = create_plots_bc_with_pathway(csv_path)\n",
    "        print(\"✓ Figure created successfully!\")\n",
    "        print(\"  ✓ Left plot (ΔG°): No pathway line\")\n",
    "        print(\"  ✓ Right plot (ΔG‡): Evolutionary pathway showing lowest ΔG‡\")\n",
    "        print(\"  ✓ Perfect vertical alignment maintained\")\n",
    "        print(\"  ✓ HUMAN MSA color scheme applied!\")\n",
    "        print(\"  ✓ Legend shows positions and pathway line!\")\n",
    "        print(\"  ✓ Figure saved as PNG to both paths!\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(\"Please check your CSV file path and column names.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a4bfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import rcParams\n",
    "import seaborn as sns\n",
    "import matplotlib.font_manager as fm\n",
    "from scipy import stats\n",
    "from scipy.stats import linregress\n",
    "import re\n",
    "import os\n",
    "\n",
    "def extract_level_number(level_str):\n",
    "    \"\"\"Extract numeric level from level string - improved version\"\"\"\n",
    "    import re\n",
    "    \n",
    "    if pd.isna(level_str):\n",
    "        return None\n",
    "    \n",
    "    # Convert to string if not already\n",
    "    level_str = str(level_str).lower().strip()\n",
    "    \n",
    "    # Try different patterns\n",
    "    patterns = [\n",
    "        r'level\\s*(\\d+)',  # level0, level 0, level_0\n",
    "        r'lvl\\s*(\\d+)',    # lvl0, lvl 0\n",
    "        r'^(\\d+)$',        # just a number\n",
    "        r'(\\d+)',          # any number in the string\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, level_str)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "def parse_value_with_error(value_str):\n",
    "    \"\"\"\n",
    "    Parse a string like '15.59 ± 0.94' and return (value, error)\n",
    "    If no error is found, return (value, 0.0)\n",
    "    \"\"\"\n",
    "    if pd.isna(value_str):\n",
    "        return None, None\n",
    "    \n",
    "    value_str = str(value_str).strip()\n",
    "    \n",
    "    # Look for pattern: number ± number\n",
    "    pattern = r'(-?\\d+\\.?\\d*)\\s*[±]\\s*(\\d+\\.?\\d*)'\n",
    "    match = re.search(pattern, value_str)\n",
    "    \n",
    "    if match:\n",
    "        value = float(match.group(1))\n",
    "        error = float(match.group(2))\n",
    "        return value, error\n",
    "    else:\n",
    "        # Try to extract just the number if no ± found\n",
    "        number_pattern = r'(-?\\d+\\.?\\d*)'\n",
    "        number_match = re.search(number_pattern, value_str)\n",
    "        if number_match:\n",
    "            value = float(number_match.group(1))\n",
    "            return value, 0.0\n",
    "        else:\n",
    "            return None, None\n",
    "\n",
    "def create_adaptive_lfer_plot_with_all_errors(csv_path, save_paths=None, error_scale_factor=1.0, \n",
    "                                             default_error=0.1, force_show_all_errors=True):\n",
    "    \"\"\"\n",
    "    Create LFER plot that shows error bars for ALL points\n",
    "    \n",
    "    Parameters:\n",
    "    - error_scale_factor: Multiplier for error bar sizes (default 1.0 = no scaling)\n",
    "    - default_error: Default error value for points without error data\n",
    "    - force_show_all_errors: If True, shows error bars for all points even if error=0\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Loaded {len(df)} rows from CSV\")\n",
    "    \n",
    "    # Show all column names to help debug\n",
    "    print(f\"\\nAll columns in CSV:\")\n",
    "    for i, col in enumerate(df.columns):\n",
    "        print(f\"  {i}: '{col}'\")\n",
    "    \n",
    "    # Check for raw data columns that might contain error information\n",
    "    raw_columns = [col for col in df.columns if 'raw' in col.lower()]\n",
    "    print(f\"\\nFound raw data columns: {raw_columns}\")\n",
    "    \n",
    "    # Parse raw data columns to extract values and errors\n",
    "    if 'dG_star_raw' in df.columns:\n",
    "        print(\"Parsing dG_star_raw column...\")\n",
    "        df[['dg_star_parsed', 'dg_star_stderr']] = df['dG_star_raw'].apply(\n",
    "            lambda x: pd.Series(parse_value_with_error(x))\n",
    "        )\n",
    "        # Show some examples\n",
    "        print(\"First 5 dG_star_raw parsing results:\")\n",
    "        for i in range(min(5, len(df))):\n",
    "            original = df.iloc[i]['dG_star_raw']\n",
    "            parsed_val = df.iloc[i]['dg_star_parsed']\n",
    "            parsed_err = df.iloc[i]['dg_star_stderr']\n",
    "            print(f\"  '{original}' -> value: {parsed_val}, error: {parsed_err}\")\n",
    "    \n",
    "    if 'dG0_raw' in df.columns:\n",
    "        print(\"\\nParsing dG0_raw column...\")\n",
    "        df[['dg0_parsed', 'dg0_stderr']] = df['dG0_raw'].apply(\n",
    "            lambda x: pd.Series(parse_value_with_error(x))\n",
    "        )\n",
    "        # Show some examples\n",
    "        print(\"First 5 dG0_raw parsing results:\")\n",
    "        for i in range(min(5, len(df))):\n",
    "            original = df.iloc[i]['dG0_raw']\n",
    "            parsed_val = df.iloc[i]['dg0_parsed']\n",
    "            parsed_err = df.iloc[i]['dg0_stderr']\n",
    "            print(f\"  '{original}' -> value: {parsed_val}, error: {parsed_err}\")\n",
    "    \n",
    "    # Standardize column names - now prioritize parsed values\n",
    "    column_mapping = {\n",
    "        'Mean_dG_star': 'dg_star_mean',\n",
    "        'Mean_dG0': 'dg0_mean', \n",
    "        'Mutation': 'mutation',\n",
    "        'mean_dg_star': 'dg_star_mean',\n",
    "        'mean_dg0': 'dg0_mean',\n",
    "    }\n",
    "    \n",
    "    for old_name, new_name in column_mapping.items():\n",
    "        if old_name in df.columns:\n",
    "            print(f\"Renaming: '{old_name}' -> '{new_name}'\")\n",
    "            df = df.rename(columns={old_name: new_name})\n",
    "    \n",
    "    # Decide which values to use - prefer parsed values from raw data\n",
    "    if 'dg_star_parsed' in df.columns and not df['dg_star_parsed'].isna().all():\n",
    "        df['dg_star'] = df['dg_star_parsed']\n",
    "        print(\"Using parsed dG_star values from raw data\")\n",
    "    elif 'dg_star_mean' in df.columns:\n",
    "        df['dg_star'] = df['dg_star_mean']\n",
    "        print(\"Using mean dG_star values\")\n",
    "    else:\n",
    "        print(\"ERROR: No dG_star column found!\")\n",
    "        return None, None, None\n",
    "    \n",
    "    if 'dg0_parsed' in df.columns and not df['dg0_parsed'].isna().all():\n",
    "        df['dg0'] = df['dg0_parsed']\n",
    "        print(\"Using parsed dG0 values from raw data\")\n",
    "    elif 'dg0_mean' in df.columns:\n",
    "        df['dg0'] = df['dg0_mean']\n",
    "        print(\"Using mean dG0 values\")\n",
    "    else:\n",
    "        print(\"ERROR: No dG0 column found!\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # IMPROVED ERROR HANDLING - ensure all points have error values\n",
    "    if 'dg_star_stderr' in df.columns:\n",
    "        # Fill NaN values with default error\n",
    "        df['dg_star_stderr'] = df['dg_star_stderr'].fillna(default_error)\n",
    "        # Replace zero errors with default error if force_show_all_errors is True\n",
    "        if force_show_all_errors:\n",
    "            df.loc[df['dg_star_stderr'] == 0, 'dg_star_stderr'] = default_error\n",
    "        has_dg_star_error = True\n",
    "    else:\n",
    "        # Create error column with default values\n",
    "        df['dg_star_stderr'] = default_error\n",
    "        has_dg_star_error = True\n",
    "    \n",
    "    # Handle dG0 errors (horizontal - disabled in original code)\n",
    "    if 'dg0_stderr' in df.columns:\n",
    "        df['dg0_stderr'] = df['dg0_stderr'].fillna(0)\n",
    "    else:\n",
    "        df['dg0_stderr'] = 0\n",
    "    \n",
    "    # Extract level numbers\n",
    "    df['Level_Numeric'] = df['Level'].apply(extract_level_number)\n",
    "    \n",
    "    # Debug: Show what levels we found\n",
    "    print(f\"\\nOriginal Level values:\")\n",
    "    for level in sorted(df['Level'].unique()):\n",
    "        count = (df['Level'] == level).sum()\n",
    "        extracted = extract_level_number(level)\n",
    "        print(f\"  '{level}' -> {extracted} ({count} rows)\")\n",
    "    \n",
    "    # Clean data\n",
    "    required_cols = ['dg_star', 'dg0', 'mutation', 'Level_Numeric']\n",
    "    df_clean = df.dropna(subset=required_cols).copy()\n",
    "    \n",
    "    # Apply error scaling factor\n",
    "    df_clean['dg_star_stderr_scaled'] = df_clean['dg_star_stderr'] * error_scale_factor\n",
    "    df_clean['dg0_stderr_scaled'] = 0  # No horizontal error bars\n",
    "    \n",
    "    print(f\"After cleaning: {len(df_clean)} rows remaining\")\n",
    "    \n",
    "    # Debug: Check error bar values\n",
    "    print(f\"\\n--- ERROR BAR VALUES DEBUG ---\")\n",
    "    print(f\"Applied scaling factor: {error_scale_factor}\")\n",
    "    print(f\"Default error for missing values: {default_error}\")\n",
    "    print(f\"Force show all errors: {force_show_all_errors}\")\n",
    "    print(f\"ΔG‡ stderr (scaled) - Min: {df_clean['dg_star_stderr_scaled'].min():.4f}, Max: {df_clean['dg_star_stderr_scaled'].max():.4f}, Mean: {df_clean['dg_star_stderr_scaled'].mean():.4f}\")\n",
    "    print(f\"Points with zero scaled error: {(df_clean['dg_star_stderr_scaled'] == 0).sum()}\")\n",
    "    print(f\"Points with non-zero scaled error: {(df_clean['dg_star_stderr_scaled'] > 0).sum()}\")\n",
    "    \n",
    "    # Get unique levels that actually exist\n",
    "    unique_levels = sorted(df_clean['Level_Numeric'].unique())\n",
    "    print(f\"Numeric levels found: {unique_levels}\")\n",
    "    \n",
    "    # Create adaptive visual mapping based on actual levels\n",
    "    n_levels = len(unique_levels)\n",
    "    \n",
    "    # Define distinct colors for levels\n",
    "    level_colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', \n",
    "                   '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "    \n",
    "    if n_levels == 1:\n",
    "        # Only one level - use single style\n",
    "        level_alphas = [0.7]\n",
    "        level_sizes = [100]\n",
    "        level_colors_mapped = [level_colors[0]]\n",
    "    else:\n",
    "        # Multiple levels - create gradients for alpha and size, distinct colors\n",
    "        level_alphas = np.linspace(0.6, 0.9, n_levels)\n",
    "        level_sizes = np.linspace(80, 120, n_levels)\n",
    "        level_colors_mapped = [level_colors[i % len(level_colors)] for i in range(n_levels)]\n",
    "    \n",
    "    # Create level to visual mapping\n",
    "    level_visual_map = {}\n",
    "    for i, level in enumerate(unique_levels):\n",
    "        level_visual_map[level] = {\n",
    "            'alpha': level_alphas[i],\n",
    "            'size': level_sizes[i],\n",
    "            'color': level_colors_mapped[i]\n",
    "        }\n",
    "    \n",
    "    # NO JITTER - use original coordinates\n",
    "    df_clean['dg0_jittered'] = df_clean['dg0']\n",
    "    df_clean['dg_star_jittered'] = df_clean['dg_star']\n",
    "    \n",
    "    # Create larger plot\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(16, 12), facecolor='white')\n",
    "    \n",
    "    # Plot by level with ALL error bars\n",
    "    for level in unique_levels:\n",
    "        level_data = df_clean[df_clean['Level_Numeric'] == level]\n",
    "        \n",
    "        # ALWAYS plot error bars for all points in this level\n",
    "        yerr = level_data['dg_star_stderr_scaled']\n",
    "        xerr = None  # No horizontal error bars\n",
    "        \n",
    "        print(f\"\\nLevel {level} - plotting error bars for ALL {len(level_data)} points\")\n",
    "        print(f\"  Y-error range: {yerr.min():.4f} to {yerr.max():.4f}\")\n",
    "        print(f\"  Points with zero error: {(yerr == 0).sum()}\")\n",
    "        print(f\"  Points with non-zero error: {(yerr > 0).sum()}\")\n",
    "        \n",
    "        # Plot error bars for ALL points (even if error is very small)\n",
    "        ax.errorbar(level_data['dg0_jittered'], level_data['dg_star_jittered'], \n",
    "                   xerr=xerr, yerr=yerr,\n",
    "                   fmt='none',  # No markers for error bars\n",
    "                   ecolor=level_visual_map[level]['color'], \n",
    "                   alpha=0.6,  # Slightly higher alpha so they're visible\n",
    "                   capsize=3,  # Slightly larger caps\n",
    "                   capthick=1.5,  # Slightly thicker caps\n",
    "                   elinewidth=1.5,  # Slightly thicker error bar lines\n",
    "                   zorder=2)  # Behind points but above grid\n",
    "        \n",
    "        # Plot the actual data points on top\n",
    "        scatter = ax.scatter(level_data['dg0_jittered'], level_data['dg_star_jittered'], \n",
    "                   c=level_visual_map[level]['color'], \n",
    "                   s=level_visual_map[level]['size'],\n",
    "                   alpha=level_visual_map[level]['alpha'],\n",
    "                   marker='o',\n",
    "                   edgecolors='black', linewidth=0.8,\n",
    "                   label=f'Level {level}',\n",
    "                   zorder=5)  # Ensure points are on top\n",
    "    \n",
    "    # Linear regression (using original non-jittered data)\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(df_clean['dg0'], df_clean['dg_star'])\n",
    "    \n",
    "    # Plot trend line\n",
    "    x_range = np.linspace(df_clean['dg0'].min(), df_clean['dg0'].max(), 100)\n",
    "    y_trend = slope * x_range + intercept\n",
    "    ax.plot(x_range, y_trend, 'r-', linewidth=2.5, alpha=0.9, zorder=3)\n",
    "    \n",
    "    # Statistics - ONLY R²\n",
    "    stats_text = f'R² = {r_value**2:.3f}'\n",
    "    \n",
    "    ax.text(0.05, 0.95, stats_text, transform=ax.transAxes, \n",
    "            verticalalignment='top', fontsize=14, \n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))\n",
    "    \n",
    "    # Labels and formatting\n",
    "    ax.set_xlabel('ΔG° (kcal/mol)', fontweight='bold', fontsize=16)\n",
    "    ax.set_ylabel('ΔG‡ (kcal/mol)', fontweight='bold', fontsize=16)\n",
    "    \n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=12)\n",
    "    \n",
    "    # Increase tick label size\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot if save_paths are provided\n",
    "    if save_paths:\n",
    "        print(f\"\\n--- SAVING PLOTS ---\")\n",
    "        for i, save_path in enumerate(save_paths):\n",
    "            try:\n",
    "                # Create directory if it doesn't exist\n",
    "                os.makedirs(save_path, exist_ok=True)\n",
    "                print(f\"Directory created/verified: {save_path}\")\n",
    "                \n",
    "                # Define filename with dataset identifier\n",
    "                dataset_name = \"HUMAN\" if \"HUMAN\" in csv_path else \"MOUSE\"\n",
    "                filename_base = f\"LFER_plot_{dataset_name}_all_errors\"\n",
    "                \n",
    "                # Save PNG\n",
    "                png_path = os.path.join(save_path, f\"{filename_base}.png\")\n",
    "                fig.savefig(png_path, dpi=300, bbox_inches='tight', \n",
    "                           facecolor='white', edgecolor='none')\n",
    "                print(f\"✓ PNG saved to: {png_path}\")\n",
    "                \n",
    "                # Save PDF\n",
    "                pdf_path = os.path.join(save_path, f\"{filename_base}.pdf\")\n",
    "                fig.savefig(pdf_path, dpi=300, bbox_inches='tight', \n",
    "                           facecolor='white', edgecolor='none')\n",
    "                print(f\"✓ PDF saved to: {pdf_path}\")\n",
    "                \n",
    "                # Save SVG\n",
    "                svg_path = os.path.join(save_path, f\"{filename_base}.svg\")\n",
    "                fig.savefig(svg_path, bbox_inches='tight', \n",
    "                           facecolor='white', edgecolor='none')\n",
    "                print(f\"✓ SVG saved to: {svg_path}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error saving to {save_path}: {e}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print comprehensive summary\n",
    "    print(f\"\\n--- ENHANCED PLOT SUMMARY ---\")\n",
    "    print(f\"Levels in your data: {unique_levels}\")\n",
    "    print(f\"Total data points: {len(df_clean)}\")\n",
    "    print(f\"Points per level:\")\n",
    "    for level in unique_levels:\n",
    "        count = (df_clean['Level_Numeric'] == level).sum()\n",
    "        print(f\"  Level {level}: {count} points\")\n",
    "    \n",
    "    # Error bar summary\n",
    "    print(f\"\\n--- ERROR BAR SUMMARY ---\")\n",
    "    print(f\"ALL POINTS NOW HAVE ERROR BARS!\")\n",
    "    print(f\"Applied scaling factor: {error_scale_factor}\")\n",
    "    print(f\"Default error for missing/zero values: {default_error}\")\n",
    "    total_points = len(df_clean)\n",
    "    zero_errors = (df_clean['dg_star_stderr_scaled'] == 0).sum()\n",
    "    nonzero_errors = (df_clean['dg_star_stderr_scaled'] > 0).sum()\n",
    "    print(f\"Points with zero error: {zero_errors}/{total_points}\")\n",
    "    print(f\"Points with non-zero error: {nonzero_errors}/{total_points}\")\n",
    "    print(f\"Mean error bar size: {df_clean['dg_star_stderr_scaled'].mean():.4f} kcal/mol\")\n",
    "    \n",
    "    return fig, ax, df_clean\n",
    "\n",
    "# Example usage with different parameters\n",
    "if __name__ == \"__main__\":\n",
    "    csv_path = \"/home/hp/results/HUMAN/distance_analysis_data.csv\"\n",
    "    save_paths = [\n",
    "        \"/home/hp/nayanika/github/GPX6/figures\",\n",
    "        \"/home/hp/nayanika/github/Article-GPX6-EVB/Figures\"\n",
    "    ]\n",
    "    \n",
    "    # Option 1: Show all error bars with default error for missing values\n",
    "    print(\"Creating plot with ALL error bars visible...\")\n",
    "    fig, ax, data = create_adaptive_lfer_plot_with_all_errors(\n",
    "        csv_path, \n",
    "        save_paths=save_paths, \n",
    "        error_scale_factor=1.0,  # No scaling\n",
    "        default_error=0.1,       # Default error for missing values\n",
    "        force_show_all_errors=True  # Force show even zero errors\n",
    "    )\n",
    "    \n",
    "    # Option 2: Alternative with smaller default error\n",
    "    # fig, ax, data = create_adaptive_lfer_plot_with_all_errors(\n",
    "    #     csv_path, \n",
    "    #     save_paths=save_paths, \n",
    "    #     error_scale_factor=0.5,  # Scale down errors\n",
    "    #     default_error=0.05,      # Smaller default error\n",
    "    #     force_show_all_errors=True\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6371c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import rcParams\n",
    "import seaborn as sns\n",
    "import matplotlib.font_manager as fm\n",
    "from scipy import stats\n",
    "from scipy.stats import linregress\n",
    "import re\n",
    "import os\n",
    "\n",
    "def extract_level_number(level_str):\n",
    "    \"\"\"Extract numeric level from level string - improved version\"\"\"\n",
    "    import re\n",
    "    \n",
    "    if pd.isna(level_str):\n",
    "        return None\n",
    "    \n",
    "    # Convert to string if not already\n",
    "    level_str = str(level_str).lower().strip()\n",
    "    \n",
    "    # Try different patterns\n",
    "    patterns = [\n",
    "        r'level\\s*(\\d+)',  # level0, level 0, level_0\n",
    "        r'lvl\\s*(\\d+)',    # lvl0, lvl 0\n",
    "        r'^(\\d+)$',        # just a number\n",
    "        r'(\\d+)',          # any number in the string\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, level_str)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "def parse_value_with_error(value_str):\n",
    "    \"\"\"\n",
    "    Parse a string like '15.59 ± 0.94' and return (value, error)\n",
    "    If no error is found, return (value, 0.0)\n",
    "    \"\"\"\n",
    "    if pd.isna(value_str):\n",
    "        return None, None\n",
    "    \n",
    "    value_str = str(value_str).strip()\n",
    "    \n",
    "    # Look for pattern: number ± number\n",
    "    pattern = r'(-?\\d+\\.?\\d*)\\s*[±]\\s*(\\d+\\.?\\d*)'\n",
    "    match = re.search(pattern, value_str)\n",
    "    \n",
    "    if match:\n",
    "        value = float(match.group(1))\n",
    "        error = float(match.group(2))\n",
    "        return value, error\n",
    "    else:\n",
    "        # Try to extract just the number if no ± found\n",
    "        number_pattern = r'(-?\\d+\\.?\\d*)'\n",
    "        number_match = re.search(number_pattern, value_str)\n",
    "        if number_match:\n",
    "            value = float(number_match.group(1))\n",
    "            return value, 0.0\n",
    "        else:\n",
    "            return None, None\n",
    "\n",
    "def create_enhanced_ddg_lfer_plot(csv_path, wt_values=None, save_paths=None, add_labels=False, error_scale_factor=0.3):\n",
    "    \"\"\"\n",
    "    Create ΔΔG‡ vs ΔΔG° LFER plot relative to wild-type with only vertical error bars\n",
    "    \n",
    "    Parameters:\n",
    "    csv_path (str): Path to the CSV file\n",
    "    wt_values (dict): Dictionary with wild-type values {'dg_star': value, 'dg0': value, 'dg_star_error': value, 'dg0_error': value}\n",
    "    save_paths (list): List of directory paths where the plot should be saved\n",
    "    add_labels (bool): Whether to add mutation labels to points (set to False)\n",
    "    error_scale_factor (float): Scale factor for error bars (0.3 = much smaller bars)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Default wild-type values if not provided (HUMAN values)\n",
    "    if wt_values is None:\n",
    "        wt_values = {\n",
    "            'dg_star': 16.40,\n",
    "            'dg0': -60.52,\n",
    "            'dg_star_error': 0.83,\n",
    "            'dg0_error': 1.46\n",
    "        }\n",
    "    \n",
    "    print(f\"Using wild-type reference values:\")\n",
    "    print(f\"  ΔG‡_WT = {wt_values['dg_star']:.2f} ± {wt_values['dg_star_error']:.2f} kcal/mol\")\n",
    "    print(f\"  ΔG°_WT = {wt_values['dg0']:.2f} ± {wt_values['dg0_error']:.2f} kcal/mol\")\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"\\nLoaded {len(df)} rows from CSV\")\n",
    "    \n",
    "    # Show all column names to help debug\n",
    "    print(f\"\\nAll columns in CSV:\")\n",
    "    for i, col in enumerate(df.columns):\n",
    "        print(f\"  {i}: '{col}'\")\n",
    "    \n",
    "    # Check for raw data columns that might contain error information\n",
    "    raw_columns = [col for col in df.columns if 'raw' in col.lower()]\n",
    "    print(f\"\\nFound raw data columns: {raw_columns}\")\n",
    "    \n",
    "    # Parse raw data columns to extract values and errors\n",
    "    if 'dG_star_raw' in df.columns:\n",
    "        print(\"Parsing dG_star_raw column...\")\n",
    "        df[['dg_star_parsed', 'dg_star_stderr']] = df['dG_star_raw'].apply(\n",
    "            lambda x: pd.Series(parse_value_with_error(x))\n",
    "        )\n",
    "        # Show some examples\n",
    "        print(\"First 5 dG_star_raw parsing results:\")\n",
    "        for i in range(min(5, len(df))):\n",
    "            original = df.iloc[i]['dG_star_raw']\n",
    "            parsed_val = df.iloc[i]['dg_star_parsed']\n",
    "            parsed_err = df.iloc[i]['dg_star_stderr']\n",
    "            print(f\"  '{original}' -> value: {parsed_val}, error: {parsed_err}\")\n",
    "    \n",
    "    if 'dG0_raw' in df.columns:\n",
    "        print(\"\\nParsing dG0_raw column...\")\n",
    "        df[['dg0_parsed', 'dg0_stderr']] = df['dG0_raw'].apply(\n",
    "            lambda x: pd.Series(parse_value_with_error(x))\n",
    "        )\n",
    "        # Show some examples\n",
    "        print(\"First 5 dG0_raw parsing results:\")\n",
    "        for i in range(min(5, len(df))):\n",
    "            original = df.iloc[i]['dG0_raw']\n",
    "            parsed_val = df.iloc[i]['dg0_parsed']\n",
    "            parsed_err = df.iloc[i]['dg0_stderr']\n",
    "            print(f\"  '{original}' -> value: {parsed_val}, error: {parsed_err}\")\n",
    "    \n",
    "    # Standardize column names - now prioritize parsed values\n",
    "    column_mapping = {\n",
    "        'Mean_dG_star': 'dg_star_mean',\n",
    "        'Mean_dG0': 'dg0_mean', \n",
    "        'Mutation': 'mutation',\n",
    "        'mean_dg_star': 'dg_star_mean',\n",
    "        'mean_dg0': 'dg0_mean',\n",
    "    }\n",
    "    \n",
    "    for old_name, new_name in column_mapping.items():\n",
    "        if old_name in df.columns:\n",
    "            print(f\"Renaming: '{old_name}' -> '{new_name}'\")\n",
    "            df = df.rename(columns={old_name: new_name})\n",
    "    \n",
    "    # Decide which values to use - prefer parsed values from raw data\n",
    "    if 'dg_star_parsed' in df.columns and not df['dg_star_parsed'].isna().all():\n",
    "        df['dg_star'] = df['dg_star_parsed']\n",
    "        print(\"Using parsed dG_star values from raw data\")\n",
    "    elif 'dg_star_mean' in df.columns:\n",
    "        df['dg_star'] = df['dg_star_mean']\n",
    "        print(\"Using mean dG_star values\")\n",
    "    else:\n",
    "        print(\"ERROR: No dG_star column found!\")\n",
    "        return None, None, None\n",
    "    \n",
    "    if 'dg0_parsed' in df.columns and not df['dg0_parsed'].isna().all():\n",
    "        df['dg0'] = df['dg0_parsed']\n",
    "        print(\"Using parsed dG0 values from raw data\")\n",
    "    elif 'dg0_mean' in df.columns:\n",
    "        df['dg0'] = df['dg0_mean']\n",
    "        print(\"Using mean dG0 values\")\n",
    "    else:\n",
    "        print(\"ERROR: No dG0 column found!\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Check for error columns\n",
    "    has_dg_star_error = 'dg_star_stderr' in df.columns and not df['dg_star_stderr'].isna().all()\n",
    "    has_dg0_error = 'dg0_stderr' in df.columns and not df['dg0_stderr'].isna().all()\n",
    "    \n",
    "    print(f\"\\nError bar availability:\")\n",
    "    print(f\"  ΔG‡ standard error: {'Available' if has_dg_star_error else 'Not found'}\")\n",
    "    print(f\"  ΔG° standard error: {'Available' if has_dg0_error else 'Not found'}\")\n",
    "    \n",
    "    # Calculate ΔΔG values relative to wild-type\n",
    "    print(f\"\\n--- CALCULATING ΔΔG VALUES ---\")\n",
    "    df['ddg_star'] = df['dg_star'] - wt_values['dg_star']\n",
    "    df['ddg0'] = df['dg0'] - wt_values['dg0']\n",
    "    \n",
    "    print(f\"ΔΔG‡ range: {df['ddg_star'].min():.2f} to {df['ddg_star'].max():.2f} kcal/mol\")\n",
    "    print(f\"ΔΔG° range: {df['ddg0'].min():.2f} to {df['ddg0'].max():.2f} kcal/mol\")\n",
    "    \n",
    "    # Calculate error propagation for ΔΔG values\n",
    "    # For differences: σ(A-B) = sqrt(σ_A² + σ_B²)\n",
    "    if has_dg_star_error:\n",
    "        df['ddg_star_stderr'] = np.sqrt(df['dg_star_stderr']**2 + wt_values['dg_star_error']**2)\n",
    "    else:\n",
    "        df['ddg_star_stderr'] = wt_values['dg_star_error']  # Use WT error if no mutation errors\n",
    "    \n",
    "    if has_dg0_error:\n",
    "        df['ddg0_stderr'] = np.sqrt(df['dg0_stderr']**2 + wt_values['dg0_error']**2)\n",
    "    else:\n",
    "        df['ddg0_stderr'] = wt_values['dg0_error']  # Use WT error if no mutation errors\n",
    "    \n",
    "    if has_dg_star_error or has_dg0_error:\n",
    "        print(f\"\\nError propagation completed:\")\n",
    "        print(f\"  ΔΔG‡ errors - Min: {df['ddg_star_stderr'].min():.4f}, Max: {df['ddg_star_stderr'].max():.4f}\")\n",
    "        print(f\"  ΔΔG° errors - Min: {df['ddg0_stderr'].min():.4f}, Max: {df['ddg0_stderr'].max():.4f}\")\n",
    "    \n",
    "    # Extract level numbers\n",
    "    df['Level_Numeric'] = df['Level'].apply(extract_level_number)\n",
    "    \n",
    "    # Debug: Show what levels we found\n",
    "    print(f\"\\nOriginal Level values:\")\n",
    "    for level in sorted(df['Level'].unique()):\n",
    "        count = (df['Level'] == level).sum()\n",
    "        extracted = extract_level_number(level)\n",
    "        print(f\"  '{level}' -> {extracted} ({count} rows)\")\n",
    "    \n",
    "    # Clean data - include error columns if they exist\n",
    "    required_cols = ['ddg_star', 'ddg0', 'mutation', 'Level_Numeric']\n",
    "    df_clean = df.dropna(subset=required_cols).copy()\n",
    "    \n",
    "    # Handle missing error values by setting them to WT error\n",
    "    if 'ddg_star_stderr' not in df_clean.columns:\n",
    "        df_clean['ddg_star_stderr'] = wt_values['dg_star_error']\n",
    "    df_clean['ddg_star_stderr'] = df_clean['ddg_star_stderr'].fillna(wt_values['dg_star_error'])\n",
    "        \n",
    "    if 'ddg0_stderr' not in df_clean.columns:\n",
    "        df_clean['ddg0_stderr'] = wt_values['dg0_error']\n",
    "    df_clean['ddg0_stderr'] = df_clean['ddg0_stderr'].fillna(wt_values['dg0_error'])\n",
    "    \n",
    "    # Apply error scaling factor (now much smaller for subtle error bars)\n",
    "    df_clean['ddg_star_stderr_scaled'] = df_clean['ddg_star_stderr'] * error_scale_factor\n",
    "    # No horizontal error bars - only vertical\n",
    "    df_clean['ddg0_stderr_scaled'] = 0\n",
    "    \n",
    "    print(f\"After cleaning: {len(df_clean)} rows remaining\")\n",
    "    \n",
    "    # Debug: Check scaled error bar values\n",
    "    print(f\"\\n--- ERROR BAR VALUES DEBUG ---\")\n",
    "    print(f\"Applied scaling factor: {error_scale_factor}\")\n",
    "    print(f\"ΔΔG‡ stderr (scaled) - Min: {df_clean['ddg_star_stderr_scaled'].min():.4f}, Max: {df_clean['ddg_star_stderr_scaled'].max():.4f}, Mean: {df_clean['ddg_star_stderr_scaled'].mean():.4f}\")\n",
    "    print(f\"ΔΔG° stderr (scaled) - DISABLED (no horizontal error bars)\")\n",
    "    \n",
    "    # Get unique levels that actually exist\n",
    "    unique_levels = sorted(df_clean['Level_Numeric'].unique())\n",
    "    print(f\"Numeric levels found: {unique_levels}\")\n",
    "    \n",
    "    # Create adaptive visual mapping based on actual levels\n",
    "    n_levels = len(unique_levels)\n",
    "    \n",
    "    # Define distinct colors for levels\n",
    "    level_colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', \n",
    "                   '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "    \n",
    "    if n_levels == 1:\n",
    "        # Only one level - use single style\n",
    "        level_alphas = [0.7]\n",
    "        level_sizes = [100]\n",
    "        level_colors_mapped = [level_colors[0]]\n",
    "    else:\n",
    "        # Multiple levels - create gradients for alpha and size, distinct colors\n",
    "        level_alphas = np.linspace(0.6, 0.9, n_levels)\n",
    "        level_sizes = np.linspace(80, 120, n_levels)\n",
    "        level_colors_mapped = [level_colors[i % len(level_colors)] for i in range(n_levels)]\n",
    "    \n",
    "    # Create level to visual mapping\n",
    "    level_visual_map = {}\n",
    "    for i, level in enumerate(unique_levels):\n",
    "        level_visual_map[level] = {\n",
    "            'alpha': level_alphas[i],\n",
    "            'size': level_sizes[i],\n",
    "            'color': level_colors_mapped[i]\n",
    "        }\n",
    "    \n",
    "    # Get MSA position colors (keeping your existing function)\n",
    "    msa_position_colors = {\n",
    "        3: \"#E41A1C\", 4: \"#377EB8\", 24: \"#E41A1C\", 47: \"#FFFF33\", 48: \"#4DAF4A\",\n",
    "        49: \"#984EA3\", 52: \"#FF7F00\", 54: \"#F781BF\", 60: \"#4DAF4A\", 74: \"#4DAF4A\",\n",
    "        87: \"#4DAF4A\", 99: \"#A65628\", 102: \"#CAB2D6\", 104: \"#FDB462\", 107: \"#00CED1\",\n",
    "        139: \"#1F78B4\", 142: \"#F781BF\", 143: \"#E31A1C\", 144: \"#FF7F00\", 173: \"#00CED1\",\n",
    "        177: \"#4DAF4A\", 178: \"#7570B3\", 181: \"#F781BF\"\n",
    "    }\n",
    "    \n",
    "    # Extract positions and assign colors\n",
    "    df_clean['Position'] = df_clean['mutation'].apply(lambda x: int(re.findall(r'\\d+', str(x))[0]) if re.findall(r'\\d+', str(x)) else None)\n",
    "    df_clean['Color'] = df_clean['Position'].map(msa_position_colors).fillna('#808080')\n",
    "    \n",
    "    # NO JITTER - use original coordinates to avoid overlaps\n",
    "    df_clean['ddg0_jittered'] = df_clean['ddg0']  # No jitter\n",
    "    df_clean['ddg_star_jittered'] = df_clean['ddg_star']  # No jitter\n",
    "    \n",
    "    # Create larger plot\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(18, 14), facecolor='white')\n",
    "    \n",
    "    # Plot by level with only vertical error bars\n",
    "    for level in unique_levels:\n",
    "        level_data = df_clean[df_clean['Level_Numeric'] == level]\n",
    "        \n",
    "        # Check if we have meaningful error values (using scaled values)\n",
    "        has_meaningful_ddg_star_err = (level_data['ddg_star_stderr_scaled'] > 0).any()\n",
    "        \n",
    "        print(f\"\\nLevel {level} error bar info:\")\n",
    "        print(f\"  Has meaningful ΔΔG‡ errors: {has_meaningful_ddg_star_err}\")\n",
    "        print(f\"  Horizontal error bars: DISABLED\")\n",
    "        \n",
    "        # Plot only vertical error bars first (so they appear behind the points)\n",
    "        if has_meaningful_ddg_star_err:\n",
    "            yerr = level_data['ddg_star_stderr_scaled']\n",
    "            xerr = None  # No horizontal error bars\n",
    "            \n",
    "            print(f\"  Plotting vertical-only error bars for level {level}\")\n",
    "            print(f\"    Y-error range: {yerr.min():.3f} to {yerr.max():.3f}\")\n",
    "            \n",
    "            ax.errorbar(level_data['ddg0_jittered'], level_data['ddg_star_jittered'], \n",
    "                       xerr=xerr, yerr=yerr,\n",
    "                       fmt='none',  # No markers for error bars\n",
    "                       ecolor=level_visual_map[level]['color'], \n",
    "                       alpha=0.5,  # Reduced alpha for subtlety\n",
    "                       capsize=2,  # Smaller cap size\n",
    "                       capthick=1,  # Thinner cap\n",
    "                       elinewidth=1,  # Thinner error bar lines\n",
    "                       zorder=2)  # Behind points but above grid\n",
    "        \n",
    "        # Plot the actual data points on top\n",
    "        scatter = ax.scatter(level_data['ddg0_jittered'], level_data['ddg_star_jittered'], \n",
    "                   c=level_visual_map[level]['color'], \n",
    "                   s=level_visual_map[level]['size'],\n",
    "                   alpha=level_visual_map[level]['alpha'],\n",
    "                   marker='o',  # All points are circles\n",
    "                   edgecolors='black', linewidth=0.8,\n",
    "                   label=f'Level {level}',\n",
    "                   zorder=5)  # Ensure points are on top\n",
    "    \n",
    "    # Add reference lines at zero (wild-type reference)\n",
    "    ax.axhline(y=0, color='gray', linestyle='--', alpha=0.7, linewidth=1, label='WT reference')\n",
    "    ax.axvline(x=0, color='gray', linestyle='--', alpha=0.7, linewidth=1)\n",
    "    \n",
    "    # Linear regression (using original non-jittered data)\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(df_clean['ddg0'], df_clean['ddg_star'])\n",
    "    \n",
    "    # Plot trend line\n",
    "    x_range = np.linspace(df_clean['ddg0'].min(), df_clean['ddg0'].max(), 100)\n",
    "    y_trend = slope * x_range + intercept\n",
    "    ax.plot(x_range, y_trend, 'r-', linewidth=2.5, alpha=0.9, zorder=3)\n",
    "    \n",
    "    # Statistics - R² only, no slope or intercept\n",
    "    stats_text = f'R² = {r_value**2:.3f}'\n",
    "    \n",
    "    ax.text(0.05, 0.95, stats_text, transform=ax.transAxes, \n",
    "            verticalalignment='top', fontsize=14, \n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))\n",
    "    \n",
    "    # Labels and formatting - Updated for ΔΔG\n",
    "    ax.set_xlabel('ΔΔG° (kcal/mol)', fontweight='bold', fontsize=18)\n",
    "    ax.set_ylabel('ΔΔG‡ (kcal/mol)', fontweight='bold', fontsize=18)\n",
    "    ax.set_title('Linear Free Energy Relationship (LFER) Plot - HUMAN', \n",
    "                 fontweight='bold', fontsize=20, pad=20)\n",
    "    \n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=12)\n",
    "    \n",
    "    # Increase tick label size\n",
    "    ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot if save_paths are provided\n",
    "    if save_paths:\n",
    "        for save_path in save_paths:\n",
    "            # Create directory if it doesn't exist\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            \n",
    "            # Define filename\n",
    "            filename = \"DDG_LFER_plot_vertical_only_HUMAN.png\"\n",
    "            full_path = os.path.join(save_path, filename)\n",
    "            \n",
    "            # Save with high quality\n",
    "            fig.savefig(full_path, dpi=300, bbox_inches='tight', \n",
    "                       facecolor='white', edgecolor='none')\n",
    "            print(f\"Enhanced ΔΔG plot saved to: {full_path}\")\n",
    "            \n",
    "            # Also save as PDF for publications\n",
    "            pdf_path = os.path.join(save_path, \"DDG_LFER_plot_vertical_only_HUMAN.pdf\")\n",
    "            fig.savefig(pdf_path, dpi=300, bbox_inches='tight', \n",
    "                       facecolor='white', edgecolor='none')\n",
    "            print(f\"PDF saved to: {pdf_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print comprehensive summary\n",
    "    print(f\"\\n--- ENHANCED ΔΔG PLOT SUMMARY ---\")\n",
    "    print(f\"Wild-type reference: humancys (ΔG‡={wt_values['dg_star']:.2f}, ΔG°={wt_values['dg0']:.2f})\")\n",
    "    print(f\"Levels in your data: {unique_levels}\")\n",
    "    print(f\"Total data points: {len(df_clean)}\")\n",
    "    print(f\"Points per level:\")\n",
    "    for level in unique_levels:\n",
    "        count = (df_clean['Level_Numeric'] == level).sum()\n",
    "        print(f\"  Level {level}: {count} points\")\n",
    "    \n",
    "    # ΔΔG ranges\n",
    "    print(f\"\\nΔΔG value ranges:\")\n",
    "    print(f\"  ΔΔG‡: {df_clean['ddg_star'].min():.2f} to {df_clean['ddg_star'].max():.2f} kcal/mol\")\n",
    "    print(f\"  ΔΔG°: {df_clean['ddg0'].min():.2f} to {df_clean['ddg0'].max():.2f} kcal/mol\")\n",
    "    \n",
    "    # Error bar summary\n",
    "    print(f\"\\n--- ERROR BAR SUMMARY ---\")\n",
    "    print(f\"Applied scaling factor: {error_scale_factor}\")\n",
    "    print(f\"ERROR BAR CONFIGURATION: VERTICAL ONLY\")\n",
    "    mean_ddg_star_err = df_clean['ddg_star_stderr_scaled'].mean()\n",
    "    max_ddg_star_err = df_clean['ddg_star_stderr_scaled'].max()\n",
    "    print(f\"ΔΔG‡ standard error (scaled) - Mean: {mean_ddg_star_err:.4f}, Max: {max_ddg_star_err:.4f} kcal/mol\")\n",
    "    print(f\"ΔΔG° standard error: DISABLED (no horizontal error bars)\")\n",
    "    \n",
    "    print(f\"\\n--- MODIFICATIONS MADE ---\")\n",
    "    print(f\"1. ✓ Converted to ΔΔG values relative to wild-type humancys\")\n",
    "    print(f\"2. ✓ Error bars: VERTICAL ONLY (with error propagation)\")\n",
    "    print(f\"3. ✓ Overlaps: REMOVED (no jitter applied)\")\n",
    "    print(f\"4. ✓ Labels: REMOVED (add_labels=False)\")\n",
    "    print(f\"5. ✓ Error scaling factor: {error_scale_factor}\")\n",
    "    print(f\"6. ✓ Added reference lines at zero (wild-type)\")\n",
    "    print(f\"7. ✓ Label shows: R² ONLY\")\n",
    "    \n",
    "    return fig, ax, df_clean\n",
    "\n",
    "# Example usage with ΔΔG values relative to wild-type:\n",
    "if __name__ == \"__main__\":\n",
    "    csv_path = \"/home/hp/results/HUMAN/distance_analysis_data.csv\"\n",
    "    save_paths = [\n",
    "        \"/home/hp/nayanika/github/GPX6/figures\",\n",
    "        \"/home/hp/nayanika/github/Article-GPX6-EVB/Figures\"\n",
    "    ]\n",
    "    \n",
    "    # Wild-type humancys values\n",
    "    wt_values = {\n",
    "        'dg_star': 16.40,\n",
    "        'dg0': -60.52,\n",
    "        'dg_star_error': 0.83,\n",
    "        'dg0_error': 1.46\n",
    "    }\n",
    "    \n",
    "    # Create plot with ΔΔG values relative to wild-type\n",
    "    print(\"Creating enhanced ΔΔG plot relative to wild-type humancys...\")\n",
    "    fig, ax, data = create_enhanced_ddg_lfer_plot(csv_path, wt_values=wt_values, \n",
    "                                                  save_paths=save_paths, \n",
    "                                                  add_labels=False, \n",
    "                                                  error_scale_factor=0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
